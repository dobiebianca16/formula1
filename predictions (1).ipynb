{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c37ee5e1-faef-48a7-bb3b-56446ca2df8e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "The execution of this command did not finish successfully",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run \"../includes/configurations\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99211bd0-0a75-4d8f-bd69-7f1be0462e3f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Drivers df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69e5b9bc-7f6d-4258-b025-faa6219a21ec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "drivers = spark.read.format(\"delta\").load(f\"{processed_folder_path}/drivers\").drop('number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce264110-bbdc-44c5-9102-f517b96a2fc7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "drivers = drivers.drop('name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b19e1f7-2ebf-4e55-b925-e4a97135671d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Results df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "062b43db-589f-40e6-b904-2278a54fed91",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "driver_result = spark.read.format(\"delta\").load(f\"{processed_folder_path}/results\").drop(\"ingestion_date\" , \"data_source\", 'number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1758c03c-e9b4-48e5-a4e5-5b7519ad127d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(driver_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1db3a99-37ac-4714-9738-771899bc3822",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Races"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3e6c82f-83fb-4556-84c0-93ba394ff9f3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "races = spark.read.format(\"delta\").load(f\"{processed_folder_path}/races\").drop(\"ingestion_date\" , \"data_source\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9857fe95-1f9e-4696-80c8-6e2a70c93fec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date, year\n",
    "\n",
    "races_date = races.select('date', 'circuit_id', 'race_id')\n",
    "\n",
    "# Eliminating 2022 and 2023\n",
    "races_date = races_date.filter((year(races_date['date']) != 2023) & (year(races_date['date']) != 2022))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2ead5ce-298e-4dda-9231-ddd044e70223",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Circuits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e24a20e-4e4d-48c6-b62d-4aa1d5ea865c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "circuits = spark.read.format(\"delta\").load(f\"{processed_folder_path}/circuits\")\n",
    "    #.drop(\"ingestion_date\" , \"data_source\")\\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35fb0462-7a2d-4cba-9a4b-66d7a5627de0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Driver Standings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4235c194-c52c-467e-8fc6-7fcad9e9ad02",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date, year\n",
    "\n",
    "driver_standings = spark.read.format(\"delta\").load(f\"{processed_folder_path}/driver_standings\")\n",
    "    #.drop(\"ingestion_date\" , \"data_source\", 'driverStandings_id')\\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcbc6f97-88a1-4741-9ff7-30f4beef334a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Lap times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5ca8174-e356-46c1-b021-84bbe0870636",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lap_times = spark.read.format(\"delta\").load(f\"{processed_folder_path}/lap_times\").withColumnRenamed('miliseconds', 'milliseconds')\n",
    "lap_times = lap_times[['race_id', 'driver_id', 'lap', 'milliseconds']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bb98693-57ae-426d-9a3d-1ad62478523d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Qualifying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e4a3094-7653-458d-959d-f888390c42ba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "qualifying = spark.read.format(\"delta\").load(f\"{processed_folder_path}/qualifying\")\n",
    "qualifying = qualifying[['driver_id', 'position']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "995db89a-947c-431c-b191-a7cd83d2b9e3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#visualization tools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "sns.set()\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "702292cf-8039-46e9-822f-080e478e9f43",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Merge Dataframes\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "driver_result_df = driver_result\n",
    "races_df = races\n",
    "drivers_df = drivers\n",
    "\n",
    "df1 = driver_result_df.join(races_df, on='race_id').dropDuplicates()\n",
    "df_race = df1.join(drivers_df, on='driver_id').dropDuplicates()\n",
    "display(df_race)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fee2974-7c9c-42ad-a523-86c970329282",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Drop posterior data column \n",
    "\n",
    "df_race = df_race.drop('laps', 'milliseconds', 'fastest_lap', 'fastest_lap_time', 'fastest_lap_speed', 'time_x', 'time_y', 'position_order' ,'ingestion_date', 'data_source')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "210c208e-5a1d-4018-b7bd-cf2e73d5b3d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_race = df_race.drop('position', 'nationality', 'name', 'position', 'position_text', 'race_timestamp', 'driver_ref' ,'time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfff2a56-52b3-4cfb-b148-d1b4d2a68bd5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_race= df_race.withColumnRenamed('race_year', 'year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ecd71eab-3e72-4781-9ad1-68f33700abd2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_race)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90f0a100-1a80-444b-bbb6-920d7cb3138c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Add age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c47d7de-bdf5-4c39-a69b-b1167c4eaa34",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import datediff, current_date, round, year\n",
    "\n",
    "\n",
    "#Add age column to dataframe\n",
    "df_race = df_race.withColumn('age', round(datediff(current_date(), 'dob') / 365))\n",
    "\n",
    "display(df_race)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2da9a408-e3e3-45ce-9dad-be2d3f40a59f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Winning a race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97c1b685-085d-48c8-b828-6717e07d5880",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import  lit\n",
    "df_driver= drivers\n",
    "#df_driver = df_driver.withColumn('totalWins', lit(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8822899-1a0c-4c6a-a87a-722b8bf00dd9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "races_date_w= races_date.select('race_id', 'date')\n",
    "driver_standings = driver_standings.join(races_date, on='race_id', how='left').dropDuplicates()\n",
    "\n",
    "display(driver_standings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65c534f4-7b1d-48d5-8d76-71417c929ce8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert the \"date\" column to a datetime object\n",
    "from pyspark.sql.functions import to_timestamp, year, lit\n",
    "\n",
    "#driver_standings = driver_standings.withColumn('date', lit(0))\n",
    "driver_standings= driver_standings.withColumn(\"date\",to_timestamp(driver_standings.date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31321b5a-01c3-44f2-b07d-237d331e4305",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#count the number of races each driven in\n",
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "num_races_per_driver_df = driver_standings.groupBy('driver_id') \\\n",
    "                                           .agg(countDistinct('race_id').alias('totalRaces'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3016c1b-29bf-4457-9b38-3dab8499fcce",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import year, max, col, count\n",
    "\n",
    "\n",
    "df_driver_final = df_driver.join(driver_standings, on='driver_id', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12cd89e3-c76b-4f04-996a-e452034e6095",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Group the dataframe by year and find the maximum date for each year\n",
    "from pyspark.sql.functions import year, max\n",
    "\n",
    "latest_day_in_year = df_driver_final.groupby((df_driver_final['driver_id']),\n",
    " df_driver_final['date']).agg({'date': 'max'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d86077f-1619-4887-b219-21dbf3fdfd44",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import year, max, sum, when\n",
    "\n",
    "# Extract year from 'date' column\n",
    "df_driver_fl = df_driver_final.withColumn('year', year('date'))\n",
    "\n",
    "# Get the latest date for each year\n",
    "latest_day_in_year = df_driver_fl.groupby('year').agg(max('date').alias('latest_date')).collect()\n",
    "\n",
    "# Convert latest_day_in_year to list of dates\n",
    "latest_day_in_year = [row.latest_date for row in latest_day_in_year]\n",
    "\n",
    "# Filter dataframe to include only rows with latest dates for each year\n",
    "filtered_dataframe = df_driver_fl.filter(col('date').isin(latest_day_in_year))\n",
    "\n",
    "# Calculate total wins for each driver\n",
    "total_wins_per_driver = filtered_dataframe.groupby('driver_id').agg(sum('wins').alias('totalWins'))\n",
    "\n",
    "# Join total_wins_per_driver with df_driver on driverId column\n",
    "df_driver = df_driver.join(total_wins_per_driver, on='driver_id', how='left_outer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1ab03d6-93c9-401d-ba93-0a30805f5d52",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# adding total races to each driver\n",
    "df_driver = df_driver.join(num_races_per_driver_df,on='driver_id', how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "319e6c73-cd8a-40b9-923d-bd57e9a0db83",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_driver = df_driver.withColumn('winRate', col('totalWins') / col('totalRaces')) \\\n",
    "                     .withColumn('age', 2023 - year('dob'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f069fef2-18cb-4113-8ba7-7c2ebd75ede5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_driver)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac5cf8af-2faf-4117-ae1d-13bc0b9b0cfc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_driver= df_driver.drop('ingestion_date', 'data_source', 'totalWins')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e16f1c6d-29a5-4237-ae25-7c21a56b35b7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###fastestLapRate - Likelihood of winning fastest lap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd61b703-3064-45cb-860f-d0353c7f8679",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, min\n",
    "\n",
    "# Group the DataFrame by 'raceId' and find the row with the minimum 'milliseconds'\n",
    "\n",
    "idx = lap_times.groupby('race_id').agg(min('milliseconds').alias('min_milliseconds')).select('race_id', 'min_milliseconds')\n",
    "#display(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63d3764d-3d6b-46f5-827d-c057a2903082",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+----+--------+-------------+----------+-------------+----------+--------------------+---+--------------------+\n|driver_id|   driver_ref|code|forename|      surname|       dob|  nationality|totalRaces|             winRate|age|      fastestLapRate|\n+---------+-------------+----+--------+-------------+----------+-------------+----------+--------------------+---+--------------------+\n|      148|       foitek|  \\N|  Gregor|       Foitek|1965-03-27|        Swiss|        32|                 0.0| 58|                 0.0|\n|      463|  chamberlain|  \\N|     Jay|  Chamberlain|1925-12-29|     American|         5|                 0.0| 98|                 0.0|\n|      471|    johnstone|  \\N|   Bruce|    Johnstone|1937-01-30|South African|         1|                 0.0| 86|                 0.0|\n|      496|   menditeguy|  \\N|  Carlos|   Menditeguy|1914-08-10|    Argentine|        62|                 0.0|109|                 0.0|\n|      833|        merhi| MER| Roberto|        Merhi|1991-03-22|      Spanish|        18|                 0.0| 32|                 0.0|\n|      243|    stommelen|  \\N|    Rolf|    Stommelen|1943-07-11|       German|        79|                 0.0| 80|                 0.0|\n|      392|       fisher|  \\N|    Mike|       Fisher|1943-03-13|     American|         4|                 0.0| 80|                 0.0|\n|      540|  mike_taylor|  \\N|    Mike|       Taylor|1934-04-24|      British|        11|                 0.0| 89|                 0.0|\n|      623|         uria|  \\N| Alberto|         Uria|1924-07-11|    Uruguayan|        15|                 0.0| 99|                 0.0|\n|      737|       obrien|  \\N|  Robert|      O'Brien|1908-04-11|     American|         6|                 0.0|115|                 0.0|\n|      858|     sargeant| SAR|   Logan|     Sargeant|2000-12-31|     American|         1|                 0.0| 23|                 0.0|\n|       31|      montoya| MON|    Juan|Pablo Montoya|1975-09-20|    Colombian|       101| 0.06930693069306931| 48| 0.12871287128712872|\n|      516|     christie|  \\N|     Bob|     Christie|1924-04-04|     American|        36|                 0.0| 99|                 0.0|\n|       85|   montermini|  \\N|  Andrea|   Montermini|1964-05-30|      Italian|        43|                 0.0| 59|                 0.0|\n|      137|       piquet|  \\N|  Nelson|       Piquet|1952-08-17|    Brazilian|       209| 0.11004784688995216| 71|                 0.0|\n|      251|ian_scheckter|  \\N|     Ian|    Scheckter|1947-08-22|South African|        57|                 0.0| 76|                 0.0|\n|      451|      dochnal|  \\N|   Frank|      Dochnal|1920-10-08|     American|         2|                 0.0|103|                 0.0|\n|      580|        godia|  \\N|    Paco|        Godia|1921-03-21|      Spanish|        21|                 0.0|102|                 0.0|\n|      808|       petrov| PET|  Vitaly|       Petrov|1984-09-08|      Russian|        58|                 0.0| 39|0.017241379310344827|\n|       65|      herbert|  \\N|  Johnny|      Herbert|1964-06-25|      British|       168|0.017857142857142856| 59|                 0.0|\n+---------+-------------+----+--------+-------------+----------+-------------+----------+--------------------+---+--------------------+\nonly showing top 20 rows\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "df_min_milliseconds = lap_times.join(idx, (lap_times.race_id == idx.race_id) & (lap_times.milliseconds == idx.min_milliseconds )).select(lap_times['*'])\n",
    "\n",
    "# Sort the result by 'raceId'\n",
    "df_min_milliseconds = df_min_milliseconds.sort('race_id')\n",
    "\n",
    "# Calculate the number of fastest laps for each 'driverId'\n",
    "counts = df_min_milliseconds.groupby('driver_id').count().select(col('driver_id'), col('count').alias('totalFastestLaps'))\n",
    "\n",
    "# Add the 'totalFastestLaps' column to 'df_driver'\n",
    "df_driver = df_driver.join(counts, on='driver_id', how='left').fillna(0)\n",
    "\n",
    "# Calculate the 'fastestLapRate'\n",
    "df_driver = df_driver.withColumn('fastestLapRate', col('totalFastestLaps') / col('totalRaces')).drop('totalFastestLaps')\n",
    "\n",
    "# Print the current DataFrame\n",
    "df_driver.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4242eea0-c4eb-425b-a6a3-fd910e75fb96",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#display(df_driver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7600bd77-89ee-4596-a7ac-97afa7dc8a32",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##fastest Qualifying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d3563f6-0b4e-4107-9d34-f7ba67ff03ce",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count\n",
    "\n",
    "# COUNTING THE NUMBER OF QUALIFYING WINS\n",
    "# Filter for position == 1 and count occurrences grouped by driverId\n",
    "position_1_counts = qualifying.filter(col('position') == 1).groupby('driver_id').agg(count('position').alias('position_1_count'))\n",
    "\n",
    "# Merge with df_driver using a left join and fill in missing values with 0\n",
    "df_driver = df_driver.join(position_1_counts, on='driver_id', how='left').fillna({'position_1_count': 0})\n",
    "\n",
    "# Calculate qualifyingWinRate and drop 'position_1_count'\n",
    "df_driver = df_driver.withColumn('qualifyingWinRate', col('position_1_count') / col('totalRaces')).drop('position_1_count')\n",
    "\n",
    "#display(df_driver)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60721787-dd9c-4ef6-ad8b-03ccfd79645c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f4de220-e3f6-40bb-a3a6-1079d62cf042",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data point for race dataframe: 25749\nNumber of data point for driver dataframe: 857\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Number of data point for race dataframe: \" + str(df_race.count()))\n",
    "print(\"Number of data point for driver dataframe: \" + str(df_driver.count()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38013c14-a293-4a83-b637-f6f3cb06aca8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data frame race information\nroot\n |-- driver_id: integer (nullable = true)\n |-- race_id: integer (nullable = true)\n |-- result_id: integer (nullable = true)\n |-- constructor_id: integer (nullable = true)\n |-- grid: integer (nullable = true)\n |-- points: float (nullable = true)\n |-- rank: integer (nullable = true)\n |-- year: integer (nullable = true)\n |-- round: integer (nullable = true)\n |-- date: date (nullable = true)\n |-- circuit_id: integer (nullable = true)\n |-- code: string (nullable = true)\n |-- forename: string (nullable = true)\n |-- surname: string (nullable = true)\n |-- dob: date (nullable = true)\n |-- age: double (nullable = true)\n\n\n\n\nData frame driver information\nroot\n |-- driver_id: integer (nullable = true)\n |-- driver_ref: string (nullable = true)\n |-- code: string (nullable = true)\n |-- forename: string (nullable = true)\n |-- surname: string (nullable = true)\n |-- dob: date (nullable = true)\n |-- nationality: string (nullable = true)\n |-- totalRaces: long (nullable = true)\n |-- winRate: double (nullable = false)\n |-- age: integer (nullable = true)\n |-- fastestLapRate: double (nullable = true)\n |-- qualifyingWinRate: double (nullable = true)\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Data frame race information\")\n",
    "df_race.printSchema()\n",
    "\n",
    "print('\\n\\n')\n",
    "print(\"Data frame driver information\")\n",
    "df_driver.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "530f8bab-085c-498d-94fe-6271a3b9817d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_race)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdbc8594-4a6f-4294-a14f-3092ef394325",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##plotting the distributions of the level variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6c2923c-0b1f-4986-b685-60db0613cbdb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Convert the PySpark dataframe to a Pandas dataframe\n",
    "df_numeric = df_race.select('grid', 'points', 'year', 'round', 'age').toPandas()\n",
    "\n",
    "# Create a subplot with 9 rows and 3 columns\n",
    "fig, axes = plt.subplots(9, 3, figsize=(20, 24))\n",
    "\n",
    "# Loop through each subplot and plot the corresponding data\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    if i < len(df_numeric.columns):\n",
    "        ax.hist(df_numeric.iloc[:, i], bins=20)\n",
    "        ax.set_xlabel(df_numeric.columns[i])\n",
    "        ax.set_ylabel('Frequency')\n",
    "    else:\n",
    "        ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ef05db7-82bd-4993-a220-5a79ea0b4559",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27c6929d-d6b5-4382-ac68-35ce8cc55a6f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Select the required columns\n",
    "df_cat = df_race.select('grid', 'points', 'age', 'rank')\n",
    "\n",
    "# Plot the distributions of the level variables\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 20))\n",
    "sns.histplot(ax=axes[0, 0], data=df_cat.toPandas(), x='grid', bins=20, kde=True)\n",
    "sns.histplot(ax=axes[0, 1], data=df_cat.toPandas(), x='points', bins=20, kde=True)\n",
    "sns.histplot(ax=axes[1, 0], data=df_cat.toPandas(), x='age', bins=20, kde=True)\n",
    "sns.histplot(ax=axes[1, 1], data=df_cat.toPandas(), x='rank', bins=20, kde=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac111199-95b0-40d5-92a6-f2e5e169db30",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Format the layout so that no overlapping between titles and graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04f5ac9f-d8e4-4943-805b-1ab8dc6857bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Convert the PySpark dataframes to Pandas dataframes\n",
    "df_numeric = df_driver.select('totalRaces', 'winRate', 'fastestLapRate', 'qualifyingWinRate').toPandas()\n",
    "df_numeric_1 = df_driver.select('totalRaces', 'winRate', 'fastestLapRate', 'qualifyingWinRate').toPandas()\n",
    "\n",
    "# Create a subplot with 8 rows and 3 columns\n",
    "fig, axes = plt.subplots(8, 3, figsize=(20, 24))\n",
    "\n",
    "# Loop through each subplot and plot the corresponding data\n",
    "count = 0\n",
    "for i in range(len(df_numeric.columns)):\n",
    "    var = df_numeric.columns[i]\n",
    "    sns.histplot(data=df_numeric[var], ax=axes[count, 0])\n",
    "    sns.boxplot(data=df_numeric[var], orient=\"h\", ax=axes[count, 1])\n",
    "    sns.violinplot(data=df_numeric[var], orient=\"h\", ax=axes[count, 2])\n",
    "    count += 1\n",
    "\n",
    "for i in range(len(df_numeric_1.columns)):\n",
    "    var = df_numeric_1.columns[i]\n",
    "    sns.histplot(data=df_numeric_1[var], ax=axes[count, 0])\n",
    "    sns.boxplot(data=df_numeric_1[var], orient=\"h\", ax=axes[count, 1])\n",
    "    sns.violinplot(data=df_numeric_1[var], orient=\"h\", ax=axes[count, 2])\n",
    "    count += 1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c39e1bd-625e-4bf0-8588-0a28295ca9ee",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Plotting the correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25ecd9f9-c78a-4fbe-9909-1dc2c8f71b03",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Select the required columns\n",
    "df_corr = df_race.select('grid', 'points', 'age', 'rank')\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "corr_matrix = np.corrcoef(df_corr.toPandas().values.T)\n",
    "corr = pd.DataFrame(data=corr_matrix, columns=['grid', 'points', 'age', 'rank'], index=['grid', 'points', 'age', 'rank'])\n",
    "\n",
    "# Plot the correlation matrix\n",
    "sns.set(style='white')\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "f, ax = plt.subplots(figsize=(7, 5))\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0, square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54a78a58-04d4-4b8e-b64e-1655197508e0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Categorical Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7fbbd87-2fbc-43fc-993c-c1ff867eb6a2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#df_race.describe().display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bcd6958-9710-471c-b49b-4d4997d87827",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "df_race_p = df_race.na.drop()\n",
    "# Convert the Spark DataFrame to a Pandas DataFrame\n",
    "df_race_p = df_race_p.toPandas()\n",
    "\n",
    "# Select the required columns\n",
    "df_cat = df_race_p[['grid', 'points', 'age', 'rank']]\n",
    "\n",
    "# Convert the selected columns to the correct data types if necessary\n",
    "df_cat['rank'] = df_cat['rank'].astype(str)\n",
    "df_cat['grid'] = df_cat['grid'].astype(int)\n",
    "df_cat['points'] = df_cat['points'].astype(int)\n",
    "df_cat['age'] = df_cat['age'].astype(int)\n",
    "\n",
    "# Create the subplots\n",
    "f, axes = plt.subplots(3, 1, figsize=(20, 30))\n",
    "\n",
    "# Loop over the columns and plot the box plots\n",
    "count = 0\n",
    "for col in df_cat.columns:\n",
    "    if col != 'rank':\n",
    "        sns.boxplot(data=df_cat, x=col, y='rank', order=['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20'], ax=axes[count])\n",
    "        count += 1\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0914c984-942a-4fc4-9a68-4dc9a369d09f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#df_race.describe().display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48d2d4c9-3243-4a2a-8db2-c11c48902693",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##heat map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27988fab-986c-40d0-8f3a-c9cebf796b5c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "df_race_temp = df_race.drop('date', 'code', 'forename', 'surname', 'dob')\n",
    "# convert all columns to double type\n",
    "df_race_temp = df_race_temp.select([F.col(c).cast(DoubleType()) for c in df_race_temp.columns])\n",
    "\n",
    "# convert Spark DataFrame to Pandas DataFrame\n",
    "pd_df = df_race_temp.toPandas()\n",
    "\n",
    "# plot the heatmap\n",
    "plt.figure(figsize=(17,12))\n",
    "sns.heatmap(pd_df.corr(),annot=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "345ce46d-ef64-4744-a5ea-d5ae275516b2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Clasify race as the first half and second half by a new variable first_half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04d334e9-5793-4c0b-8be6-585c4624ba9b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import month, col, sum, avg, when\n",
    "from pyspark.sql import Row\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = df_race\n",
    "\n",
    "# Calculate the firstHalf column with real values\n",
    "df = df.withColumn(\"date\", df[\"date\"].cast(\"date\"))\n",
    "df = df.withColumn(\"firstHalf\", (month(col(\"date\")) <= 6).cast(\"integer\"))\n",
    "\n",
    "# Group by year, first half, and driverId and calculate the sum of points\n",
    "point_year_divided = df.groupby(\"year\", \"firstHalf\", \"driver_id\")\\\n",
    "    .agg((when(col(\"firstHalf\") == 0, sum(\"points\")).otherwise('')).alias(\"points\"))\n",
    "\n",
    "# Calculate the average age for each year, first half, and driverId\n",
    "age_year = df.groupby(\"year\", \"firstHalf\", \"driver_id\").agg(avg(\"age\").alias(\"avg_age\"), F.mean(\"points\").alias(\"points\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a347d21-4114-4fb7-bc0c-f121edcf0480",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(point_year_divided)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd1329d2-a583-4553-9f7c-e00728572fc7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "point_first_half_all = []\n",
    "whole_year_point_all = []\n",
    "\n",
    "for column in point_year_divided.columns[3:]:\n",
    "    point_year_driver_divided = point_year_divided.select(\"year\", \"firstHalf\", \"driver_id\", column).dropna(subset=[column])\n",
    "    age_one_year = age_year.select(\"year\", \"firstHalf\", \"driver_id\", \"avg_age\").join(point_year_driver_divided, [\"year\", \"firstHalf\", \"driver_id\"], \"inner\")\n",
    "\n",
    "    new_df = age_one_year.withColumn(\"ages\", when(col(\"avg_age\").isNull(), col(column)).otherwise(col(\"avg_age\"))) \\\n",
    "        .select(\"firstHalf\", \"ages\", \"points\", (col(column) + col(\"points\")).alias(\"whole_year_point\"))\n",
    "\n",
    "    point_first_half_all.extend(new_df.select(\"firstHalf\", \"ages\", \"points\").rdd.collect())\n",
    "    whole_year_point_all.extend(new_df.select(\"whole_year_point\", \"ages\", \"firstHalf\").rdd.collect())\n",
    "\n",
    "# Convert Python lists to DataFrames\n",
    "point_first_half_all_df = spark.createDataFrame([Row(firstHalf=row[0], ages=row[1], points=row[2]) for row in point_first_half_all])\n",
    "whole_year_point_all_df = spark.createDataFrame([Row(whole_year_point=row[0], ages=row[1], firstHalf=row[2]) for row in whole_year_point_all])\n",
    "\n",
    "# Plot the scatterplot using matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(point_first_half_all_df.select(\"points\").rdd.flatMap(lambda x: x).collect(),\n",
    "            whole_year_point_all_df.select(\"whole_year_point\").rdd.flatMap(lambda x: x).collect())\n",
    "plt.xlabel(\"First Half Points\")\n",
    "plt.ylabel(\"Whole Year Points\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4410f3f-d8f2-4418-8993-4637530bb8c9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(point_first_half_all_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f95b6165-2b72-44c7-81a0-9a80e51e1187",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Supervised method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09d83ddd-5330-43ec-bc49-3b30e32243f2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13f528f7-ba52-49ce-8b03-6eab9ae7f8db",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goodness of Fit of Model \tTrain Dataset\nExplained Variance (R^2) \t: 1.8540724511240114e-14\nMean Squared Error (MSE) \t: 3722.9868754835215\n\nGoodness of Fit of Model and Prediction Accuracy \tTest Dataset\nExplained Variance (R^2) \t: -0.001432018698080073\nMean Squared Error (MSE) \t: 3397.7708448221374\n\nIntercept of Regression \t: b = 23.08042676996884\nCoefficients of Regression \t: a = [0.0]\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "new_df_1 = new_df.na.drop()\n",
    "# Train and Test Split\n",
    "train_df, test_df = new_df_1.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Prepare the feature column\n",
    "feature_cols = [\"firstHalf\"]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "train_features = assembler.transform(train_df)\n",
    "test_features = assembler.transform(test_df)\n",
    "\n",
    "# Prepare the target column\n",
    "target_col = \"whole_year_point\"\n",
    "train_target = train_df.select(target_col)\n",
    "test_target = test_df.select(target_col)\n",
    "\n",
    "# Train the Linear Regression model\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=target_col, maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "lr_model = lr.fit(train_features)\n",
    "\n",
    "# Make predictions on the train and test data\n",
    "train_predictions = lr_model.transform(train_features)\n",
    "test_predictions = lr_model.transform(test_features)\n",
    "\n",
    "# Create the evaluator\n",
    "evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=target_col)\n",
    "\n",
    "# Train dataset evaluation\n",
    "train_r2 = evaluator.evaluate(train_predictions, {evaluator.metricName: \"r2\"})\n",
    "train_mse = evaluator.evaluate(train_predictions, {evaluator.metricName: \"mse\"})\n",
    "\n",
    "# Test dataset evaluation\n",
    "test_r2 = evaluator.evaluate(test_predictions, {evaluator.metricName: \"r2\"})\n",
    "test_mse = evaluator.evaluate(test_predictions, {evaluator.metricName: \"mse\"})\n",
    "\n",
    "# Print evaluation results\n",
    "print(\"Goodness of Fit of Model \\tTrain Dataset\")\n",
    "print(\"Explained Variance (R^2) \\t:\", train_r2)\n",
    "print(\"Mean Squared Error (MSE) \\t:\", train_mse)\n",
    "print()\n",
    "\n",
    "print(\"Goodness of Fit of Model and Prediction Accuracy \\tTest Dataset\")\n",
    "print(\"Explained Variance (R^2) \\t:\", test_r2)\n",
    "print(\"Mean Squared Error (MSE) \\t:\", test_mse)\n",
    "print()\n",
    "\n",
    "# Print intercept and coefficients\n",
    "print(\"Intercept of Regression \\t: b =\", lr_model.intercept)\n",
    "print(\"Coefficients of Regression \\t: a =\", lr_model.coefficients)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40e0182c-b00c-4eab-9c20-ee2eb1cc35cc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32634e64-6b10-4aec-b992-df5b2b01c744",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goodness of Fit of Model \tTrain Dataset\nExplained Variance (R^2) \t: 2.4424906541753444e-15\nMean Squared Error (MSE) \t: 3693.029224305617\n\nGoodness of Fit of Model and Prediction Accuracy \tTest Dataset\nExplained Variance (R^2) \t: -0.0013070970115658387\nMean Squared Error (MSE) \t: 3534.999721331512\nIntercept of Regression \t: b = 22.30073714857536\nCoefficients of Regression \t: a = [0.0]\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_liniar = new_df\n",
    "data = df_liniar.select(\"firstHalf\", \"whole_year_point\")\n",
    "data = data.withColumnRenamed(\"firstHalf\", \"features\")\n",
    "data = data.withColumnRenamed(\"whole_year_point\", \"label\")\n",
    "data = data.filter(col(\"features\").isNotNull() & col(\"label\").isNotNull())  # Filter out rows with null values\n",
    "train_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\n",
    "assembler = VectorAssembler(inputCols=[\"features\"], outputCol=\"features_vec\")\n",
    "train_data = assembler.transform(train_data)\n",
    "test_data = assembler.transform(test_data)\n",
    "lr = LinearRegression(featuresCol=\"features_vec\", labelCol=\"label\")\n",
    "lr_model = lr.fit(train_data)\n",
    "train_predictions = lr_model.transform(train_data)\n",
    "test_predictions = lr_model.transform(test_data)\n",
    "evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "\n",
    "# Train dataset evaluation\n",
    "train_r2 = evaluator.evaluate(train_predictions, {evaluator.metricName: \"r2\"})\n",
    "train_mse = evaluator.evaluate(train_predictions, {evaluator.metricName: \"mse\"})\n",
    "\n",
    "# Test dataset evaluation\n",
    "test_r2 = evaluator.evaluate(test_predictions, {evaluator.metricName: \"r2\"})\n",
    "test_mse = evaluator.evaluate(test_predictions, {evaluator.metricName: \"mse\"})\n",
    "print(\"Goodness of Fit of Model \\tTrain Dataset\")\n",
    "print(\"Explained Variance (R^2) \\t:\", train_r2)\n",
    "print(\"Mean Squared Error (MSE) \\t:\", train_mse)\n",
    "print()\n",
    "\n",
    "print(\"Goodness of Fit of Model and Prediction Accuracy \\tTest Dataset\")\n",
    "print(\"Explained Variance (R^2) \\t:\", test_r2)\n",
    "print(\"Mean Squared Error (MSE) \\t:\", test_mse)\n",
    "\n",
    "print(\"Intercept of Regression \\t: b =\", lr_model.intercept)\n",
    "print(\"Coefficients of Regression \\t: a =\", lr_model.coefficients)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b114fe22-dd6b-4132-91f7-de8938385c55",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97721962-4250-4194-8d60-4694a1af177b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goodness of Fit of Model \tTrain Dataset\nExplained Variance (R^2) \t: 2.4424906541753444e-15\nMean Squared Error (MSE) \t: 3693.029224305617\n\nGoodness of Fit of Model and Prediction Accuracy \tTest Dataset\nExplained Variance (R^2) \t: -0.0013070970115658387\nMean Squared Error (MSE) \t: 3534.999721331512\nIntercept of Regression \t: b = 22.30073714857536\nCoefficients of Regression \t: a = [0.0,0.0]\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler, PolynomialExpansion\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "\n",
    "# Load the data\n",
    "df_polynomial = new_df.na.drop()\n",
    "\n",
    "# Prepare the data\n",
    "data = df_polynomial.select(\"firstHalf\", \"whole_year_point\")\n",
    "data = data.withColumnRenamed(\"firstHalf\", \"features\")\n",
    "data = data.withColumnRenamed(\"whole_year_point\", \"label\")\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Create the feature vector assembler\n",
    "assembler = VectorAssembler(inputCols=[\"features\"], outputCol=\"features_vec\")\n",
    "train_data = assembler.transform(train_data)\n",
    "test_data = assembler.transform(test_data)\n",
    "\n",
    "# Perform polynomial expansion\n",
    "polynomial_expansion = PolynomialExpansion(inputCol=\"features_vec\", outputCol=\"poly_features\", degree=2)\n",
    "train_data_poly = polynomial_expansion.transform(train_data)\n",
    "test_data_poly = polynomial_expansion.transform(test_data)\n",
    "\n",
    "# Create the linear regression model\n",
    "lr = LinearRegression(featuresCol=\"poly_features\", labelCol=\"label\")\n",
    "lr_model = lr.fit(train_data_poly)\n",
    "\n",
    "# Make predictions on train and test data\n",
    "train_predictions = lr_model.transform(train_data_poly)\n",
    "test_predictions = lr_model.transform(test_data_poly)\n",
    "\n",
    "# Create the evaluation object\n",
    "evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "\n",
    "# Train dataset evaluation\n",
    "train_r2 = evaluator.evaluate(train_predictions, {evaluator.metricName: \"r2\"})\n",
    "train_mse = evaluator.evaluate(train_predictions, {evaluator.metricName: \"mse\"})\n",
    "\n",
    "# Test dataset evaluation\n",
    "test_r2 = evaluator.evaluate(test_predictions, {evaluator.metricName: \"r2\"})\n",
    "test_mse = evaluator.evaluate(test_predictions, {evaluator.metricName: \"mse\"})\n",
    "\n",
    "# Print the results\n",
    "print(\"Goodness of Fit of Model \\tTrain Dataset\")\n",
    "print(\"Explained Variance (R^2) \\t:\", train_r2)\n",
    "print(\"Mean Squared Error (MSE) \\t:\", train_mse)\n",
    "print()\n",
    "\n",
    "print(\"Goodness of Fit of Model and Prediction Accuracy \\tTest Dataset\")\n",
    "print(\"Explained Variance (R^2) \\t:\", test_r2)\n",
    "print(\"Mean Squared Error (MSE) \\t:\", test_mse)\n",
    "\n",
    "print(\"Intercept of Regression \\t: b =\", lr_model.intercept)\n",
    "print(\"Coefficients of Regression \\t: a =\", lr_model.coefficients)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb515b95-20c5-49f0-b236-bbd4d0e43916",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Second track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c703c2f-5d92-4220-b03a-d75d99309ee2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "driver_result_withdate_divided = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "845f421f-ecda-4e4c-be0e-b8d562355a39",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#looop\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Filter the DataFrame\n",
    "driver_result_withdate_divided = driver_result_withdate_divided.filter(col(\"points\") <= 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3db5a615-8dd1-4cef-975c-821ff8a8e622",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extract unique driver IDs\n",
    "driver_ids = driver_result_withdate_divided.select(\"driver_id\").distinct().rdd.flatMap(lambda x: x).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c80307b8-dd12-4846-b55c-ae4bbdf3e8ee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, avg, expr, lag\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "last_ten_race_results = []\n",
    "current_race_results = []\n",
    "desired_iterations = 50  # Specify the desired number of iterations\n",
    "\n",
    "for driver_id in driver_ids:\n",
    "    temp_df = driver_result_withdate_divided.filter(col(\"driver_id\") == driver_id)\n",
    "    temp_df = temp_df.sort(\"date\")\n",
    "    \n",
    "    if temp_df.count() <= (desired_iterations + 10):  # Check if the number of rows is sufficient\n",
    "        continue\n",
    "    \n",
    "    windowSpec = Window.orderBy(\"date\")\n",
    "    temp_df = temp_df.withColumn(\"points_lag\", lag(col(\"points\")).over(windowSpec))\n",
    "    temp_df = temp_df.withColumn(\"age_lag\", lag(col(\"age\")).over(windowSpec))\n",
    "    temp_df = temp_df.withColumn(\"round_lag\", lag(col(\"round\")).over(windowSpec))\n",
    "    temp_df = temp_df.withColumn(\"grid_lag\", lag(col(\"grid\")).over(windowSpec))\n",
    "    temp_df = temp_df.withColumn(\"raceId_lag\", lag(col(\"race_id\")).over(windowSpec))\n",
    "    \n",
    "    temp_df = temp_df.filter(col(\"points_lag\").isNotNull())\n",
    "    \n",
    "    temp_df = temp_df.withColumn(\"last_ten_race_results\", expr(\"array(points_lag, age_lag, round_lag, grid_lag, raceId_lag)\"))\n",
    "    temp_df = temp_df.withColumn(\"current_race_result\", avg(col(\"points\")).over(Window.partitionBy().orderBy(\"date\").rowsBetween(-9, 0)))\n",
    "    \n",
    "    temp_df = temp_df.filter(col(\"current_race_result\").isNotNull())\n",
    "    \n",
    "    last_ten_race_results.extend(temp_df.select(\"last_ten_race_results\").collect())\n",
    "    current_race_results.extend(temp_df.select(\"current_race_result\").collect())\n",
    "    \n",
    "    if len(last_ten_race_results) >= desired_iterations:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a68c7792-7abb-46ce-83d0-9e31191886d5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "assert len(last_ten_race_results) == len(current_race_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42cb5d06-69a4-48b8-b415-c17bb121fa8b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#split the data\n",
    "from pyspark.sql.functions import rand\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "# Combine the features and labels into a single DataFrame\n",
    "data = zip(last_ten_race_results, current_race_results)\n",
    "data_df = spark.createDataFrame(data, [\"features\", \"label\"])\n",
    "\n",
    "# Randomly split the data into train and test sets\n",
    "train_ratio = 0.8\n",
    "test_ratio = 1 - train_ratio\n",
    "seed = 42  # Set a seed for reproducibility\n",
    "\n",
    "train_data, test_data = data_df.randomSplit([train_ratio, test_ratio], seed=seed)\n",
    "\n",
    "# Extract the feature and label columns from the train and test sets\n",
    "train_x = train_data.select(\"features\").rdd.map(lambda x: x[0]).collect()\n",
    "train_y = train_data.select(\"label\").rdd.map(lambda x: x[0]).collect()\n",
    "\n",
    "test_x = test_data.select(\"features\").rdd.map(lambda x: x[0]).collect()\n",
    "test_y = test_data.select(\"label\").rdd.map(lambda x: x[0]).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a1da979-cb09-4536-bddd-5c1ed5d7af89",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data: 0.374\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1599: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Convert train_x and train_y to numpy arrays\n",
    "train_x = np.array(train_x)\n",
    "train_y = np.array(train_y)\n",
    "\n",
    "# Reshape train_x to 2D array\n",
    "train_x = train_x.reshape((train_x.shape[0], -1))\n",
    "\n",
    "# Create and train the MLPRegressor\n",
    "mlp_regressor = MLPRegressor(hidden_layer_sizes=(30, 20, 30, 10, 20, 40), max_iter=300)\n",
    "mlp_regressor.fit(train_x, train_y)\n",
    "\n",
    "# Convert test_x to numpy array\n",
    "test_x = np.array(test_x)\n",
    "\n",
    "# Reshape test_x to 2D array\n",
    "test_x = test_x.reshape((test_x.shape[0], -1))\n",
    "\n",
    "# Make predictions on the test set\n",
    "test_predictions = mlp_regressor.predict(test_x)\n",
    "\n",
    "# Evaluate the model using RMSE\n",
    "rmse = mean_squared_error(test_y, test_predictions, squared=False)\n",
    "print(f\"Root Mean Squared Error (RMSE) on test data: {rmse:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41f16217-3ebd-447c-b7bd-b228ba3993e7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /databricks/python3/lib/python3.9/site-packages (1.0.2)\r\nRequirement already satisfied: threadpoolctl>=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from scikit-learn) (2.2.0)\r\nRequirement already satisfied: scipy>=1.1.0 in /databricks/python3/lib/python3.9/site-packages (from scikit-learn) (1.7.3)\r\nRequirement already satisfied: joblib>=0.11 in /databricks/python3/lib/python3.9/site-packages (from scikit-learn) (1.1.1)\r\nRequirement already satisfied: numpy>=1.14.6 in /databricks/python3/lib/python3.9/site-packages (from scikit-learn) (1.21.5)\r\n\u001B[33mWARNING: You are using pip version 21.2.4; however, version 23.1.2 is available.\r\nYou should consider upgrading via the '/local_disk0/.ephemeral_nfs/envs/pythonEnv-2e899ed4-198d-4b7d-b9ed-2f42f1e48747/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f18953f4-188c-4c65-b9ce-3e6fc43b36bb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goodness of Fit of Model \tTrain Dataset\nR2 Score \t: -0.005174330627510493\nMean Squared Error (MSE) \t: 0.13079861999992706\n\nGoodness of Fit of Model and Prediction Accuracy \tTest Dataset\nR2 Score \t: -0.003530651304948673\nMean Squared Error (MSE) \t: 0.13926611024575306\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1599: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Create and train the MLPRegressor\n",
    "mlp_regressor = MLPRegressor(hidden_layer_sizes=(30, 20, 30, 10, 20, 40), max_iter=300)\n",
    "mlp_regressor.fit(train_x, train_y)\n",
    "\n",
    "# Make predictions on the train set\n",
    "train_predictions = mlp_regressor.predict(train_x)\n",
    "\n",
    "# Check the Goodness of Fit (on Train Data)\n",
    "print(\"Goodness of Fit of Model \\tTrain Dataset\")\n",
    "print(\"R2 Score \\t:\", mlp_regressor.score(train_x, train_y))\n",
    "print(\"Mean Squared Error (MSE) \\t:\", mean_squared_error(train_y, train_predictions))\n",
    "print()\n",
    "\n",
    "# Make predictions on the test set\n",
    "test_predictions = mlp_regressor.predict(test_x)\n",
    "\n",
    "# Check the Goodness of Fit and Prediction Accuracy (on Test Data)\n",
    "print(\"Goodness of Fit of Model and Prediction Accuracy \\tTest Dataset\")\n",
    "print(\"R2 Score \\t:\", mlp_regressor.score(test_x, test_y))\n",
    "print(\"Mean Squared Error (MSE) \\t:\", mean_squared_error(test_y, test_predictions))\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "predictions (1)",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
